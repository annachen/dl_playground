# Dataset
dataset: mnist_naive_edge_graph

# Training config
train_config:
  batch_size: 32
  epochs: 100
  eval_every_n_steps: 500
  summary_every_n_steps: 20
  save_every_n_steps: 1000
  max_ckpts_to_keep: 20
  shuffle_buffer: 500

# Learning rate
optimizer: adam
learning_rate:
  initial_learning_rate: 0.001
  decay_steps: 1000
  decay_rate: 0.97
  staircase: true

# networks
message_mlp:
  filters: [8]
  last_layer_act_fn: tanh
  use_leaky: true

node_update_mlp:
  filters: [16]
  last_layer_act_fn: tanh
  use_leaky: true

gnn_layer:
  Dn: 16
  message_gather: sum

output_mlp:
  filters: [10]
  last_layer_act_fn: linear

gnn_global_classifier:
  n_layers: 4
  n_classes: 10
  Din: 4

load_checkpoint: null
